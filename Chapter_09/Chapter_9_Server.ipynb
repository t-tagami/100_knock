{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8889/notebooks/t-tagami/Chapter_9/Chapter_9_Server.ipynb#第9章:-ベクトル空間法-(I)\" data-toc-modified-id=\"第9章:-ベクトル空間法-(I)-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>第9章: ベクトル空間法 (I)</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8889/notebooks/t-tagami/Chapter_9/Chapter_9_Server.ipynb#80.-コーパスの整形\" data-toc-modified-id=\"80.-コーパスの整形-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>80. コーパスの整形</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/t-tagami/Chapter_9/Chapter_9_Server.ipynb#81.-複合語からなる国名への対処\" data-toc-modified-id=\"81.-複合語からなる国名への対処-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>81. 複合語からなる国名への対処</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/t-tagami/Chapter_9/Chapter_9_Server.ipynb#82.-文脈の抽出\" data-toc-modified-id=\"82.-文脈の抽出-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>82. 文脈の抽出</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/t-tagami/Chapter_9/Chapter_9_Server.ipynb#時間計測\" data-toc-modified-id=\"時間計測-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>時間計測</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/t-tagami/Chapter_9/Chapter_9_Server.ipynb#83.-単語／文脈の頻度の計測\" data-toc-modified-id=\"83.-単語／文脈の頻度の計測-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>83. 単語／文脈の頻度の計測</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/t-tagami/Chapter_9/Chapter_9_Server.ipynb#メモリ使用量計測\" data-toc-modified-id=\"メモリ使用量計測-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>メモリ使用量計測</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/t-tagami/Chapter_9/Chapter_9_Server.ipynb#84.-単語文脈行列の作成\" data-toc-modified-id=\"84.-単語文脈行列の作成-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>84. 単語文脈行列の作成</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/t-tagami/Chapter_9/Chapter_9_Server.ipynb#85.-主成分分析による次元圧縮\" data-toc-modified-id=\"85.-主成分分析による次元圧縮-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>85. 主成分分析による次元圧縮</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/t-tagami/Chapter_9/Chapter_9_Server.ipynb#86.-単語ベクトルの表示\" data-toc-modified-id=\"86.-単語ベクトルの表示-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>86. 単語ベクトルの表示</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/t-tagami/Chapter_9/Chapter_9_Server.ipynb#87.-単語の類似度\" data-toc-modified-id=\"87.-単語の類似度-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>87. 単語の類似度</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/t-tagami/Chapter_9/Chapter_9_Server.ipynb#88.-類似度の高い単語10件\" data-toc-modified-id=\"88.-類似度の高い単語10件-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>88. 類似度の高い単語10件</a></span></li><li><span><a href=\"http://localhost:8889/notebooks/t-tagami/Chapter_9/Chapter_9_Server.ipynb#89.-加法構成性によるアナロジー\" data-toc-modified-id=\"89.-加法構成性によるアナロジー-1.12\"><span class=\"toc-item-num\">1.12&nbsp;&nbsp;</span>89. 加法構成性によるアナロジー</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第9章: ベクトル空間法 (I)\n",
    "enwiki-20150112-400-r10-105752.txt.bz2は，2015年1月12日時点の英語のWikipedia記事のうち，約400語以上で構成される記事の中から，ランダムに1/10サンプリングした105,752記事のテキストをbzip2形式で圧縮したものである．このテキストをコーパスとして，単語の意味を表すベクトル（分散表現）を学習したい．第9章の前半では，コーパスから作成した単語文脈共起行列に主成分分析を適用し，単語ベクトルを学習する過程を，いくつかの処理に分けて実装する．第9章の後半では，学習で得られた単語ベクトル（300次元）を用い，単語の類似度計算やアナロジー（類推）を行う．\n",
    "\n",
    "なお，問題83を素直に実装すると，大量（約7GB）の主記憶が必要になる． メモリが不足する場合は，処理を工夫するか，1/100サンプリングのコーパスenwiki-20150112-400-r100-10576.txt.bz2を用いよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-01-08 18:59:41--  http://www.cl.ecei.tohoku.ac.jp/nlp100/data/enwiki-20150112-400-r10-105752.txt.bz2\n",
      "Resolving www.cl.ecei.tohoku.ac.jp (www.cl.ecei.tohoku.ac.jp)... 130.34.192.83\n",
      "Connecting to www.cl.ecei.tohoku.ac.jp (www.cl.ecei.tohoku.ac.jp)|130.34.192.83|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 221903910 (212M) [application/x-bzip2]\n",
      "Saving to: ‘data/enwiki-20150112-400-r10-105752.txt.bz2’\n",
      "\n",
      "enwiki-20150112-400 100%[===================>] 211.62M  58.0MB/s    in 3.9s    \n",
      "\n",
      "2018-01-08 18:59:47 (54.1 MB/s) - ‘data/enwiki-20150112-400-r10-105752.txt.bz2’ saved [221903910/221903910]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P data http://www.cl.ecei.tohoku.ac.jp/nlp100/data/enwiki-20150112-400-r10-105752.txt.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2875326\r\n"
     ]
    }
   ],
   "source": [
    "!bzcat data/enwiki-20150112-400-r10-105752.txt.bz2 | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80. コーパスの整形\n",
    "文を単語列に変換する最も単純な方法は，空白文字で単語に区切ることである． ただ，この方法では文末のピリオドや括弧などの記号が単語に含まれてしまう． そこで，コーパスの各行のテキストを空白文字でトークンのリストに分割した後，各トークンに以下の処理を施し，単語から記号を除去せよ．\n",
    "* トークンの先頭と末尾に出現する次の文字を削除: .,!?;:()[]'\"\n",
    "* 空文字列となったトークンは削除\n",
    "以上の処理を適用した後，トークンをスペースで連結してファイルに保存せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/80.py\n"
     ]
    }
   ],
   "source": [
    "%%file src/80.py\n",
    "import bz2\n",
    "with bz2.open('data/enwiki-20150112-400-r10-105752.txt.bz2', 'rt') as f_r:\n",
    "    for line in f_r:\n",
    "        tokens = [word.strip(\".,!?;:()[]'\\\"\") for word in line.split()]\n",
    "        print(' '.join(token for token in tokens if token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python src/80.py > work/80.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism\r\n",
      "\r\n",
      "Anarchism is a political philosophy that advocates stateless societies often defined as self-governed voluntary institutions but that several authors have defined as more specific institutions based on non-hierarchical free associations Anarchism holds the state to be undesirable unnecessary or harmful While anti-statism is central anarchism entails opposing authority or hierarchical organisation in the conduct of human relations including but not limited to the state system\r\n"
     ]
    }
   ],
   "source": [
    "!head -3 work/80.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 81. 複合語からなる国名への対処\n",
    "英語では，複数の語の連接が意味を成すことがある．例えば，アメリカ合衆国は\"United States\"，イギリスは\"United Kingdom\"と表現されるが，\"United\"や\"States\"，\"Kingdom\"という単語だけでは，指し示している概念・実体が曖昧である．そこで，コーパス中に含まれる複合語を認識し，複合語を1語として扱うことで，複合語の意味を推定したい．しかしながら，複合語を正確に認定するのは大変むずかしいので，ここでは複合語からなる国名を認定したい．\n",
    "\n",
    "インターネット上から国名リストを各自で入手し，80のコーパス中に出現する複合語の国名に関して，スペースをアンダーバーに置換せよ．例えば，\"United States\"は\"United_States\"，\"Isle of Man\"は\"Isle_of_Man\"になるはずである．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycountry\n",
      "  Downloading pycountry-17.9.23.tar.gz (9.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 9.2MB 98kB/s eta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: pycountry\n",
      "  Running setup.py bdist_wheel for pycountry ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/tagami/.cache/pip/wheels/95/45/76/aa36bb308428b450eb1f51955b0f26911f00df3c85602c1da8\n",
      "Successfully built pycountry\n",
      "Installing collected packages: pycountry\n",
      "Successfully installed pycountry-17.9.23\n"
     ]
    }
   ],
   "source": [
    "!pip install pycountry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Åland Islands', 'United Arab Emirates', 'American Samoa', 'French Southern Territories', 'Antigua and Barbuda', 'Bonaire, Sint Eustatius and Saba', 'Burkina Faso', 'Bosnia and Herzegovina', 'Saint Barthélemy', 'Bolivia, Plurinational State of', 'Brunei Darussalam', 'Bouvet Island', 'Central African Republic', 'Cocos (Keeling) Islands', \"Côte d'Ivoire\", 'Congo, The Democratic Republic of the', 'Cook Islands', 'Cabo Verde', 'Costa Rica', 'Christmas Island', 'Cayman Islands', 'Dominican Republic', 'Western Sahara', 'Falkland Islands (Malvinas)', 'Faroe Islands', 'Micronesia, Federated States of', 'United Kingdom', 'Equatorial Guinea', 'French Guiana', 'Hong Kong', 'Heard Island and McDonald Islands', 'Isle of Man', 'British Indian Ocean Territory', 'Iran, Islamic Republic of', 'Saint Kitts and Nevis', 'Korea, Republic of', \"Lao People's Democratic Republic\", 'Saint Lucia', 'Sri Lanka', 'Saint Martin (French part)', 'Moldova, Republic of', 'Marshall Islands', 'Macedonia, Republic of', 'Northern Mariana Islands', 'New Caledonia', 'Norfolk Island', 'New Zealand', 'Papua New Guinea', 'Puerto Rico', \"Korea, Democratic People's Republic of\", 'Palestine, State of', 'French Polynesia', 'Russian Federation', 'Saudi Arabia', 'South Georgia and the South Sandwich Islands', 'Saint Helena, Ascension and Tristan da Cunha', 'Svalbard and Jan Mayen', 'Solomon Islands', 'Sierra Leone', 'El Salvador', 'San Marino', 'Saint Pierre and Miquelon', 'South Sudan', 'Sao Tome and Principe', 'Sint Maarten (Dutch part)', 'Syrian Arab Republic', 'Turks and Caicos Islands', 'Trinidad and Tobago', 'Taiwan, Province of China', 'Tanzania, United Republic of', 'United States Minor Outlying Islands', 'United States', 'Holy See (Vatican City State)', 'Saint Vincent and the Grenadines', 'Venezuela, Bolivarian Republic of', 'Virgin Islands, British', 'Virgin Islands, U.S.', 'Viet Nam', 'Wallis and Futuna', 'South Africa']\n"
     ]
    }
   ],
   "source": [
    "import pycountry\n",
    "country_list = [country.name for country in pycountry.countries if len(country.name.split()) > 1]\n",
    "print(country_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/81.py\n"
     ]
    }
   ],
   "source": [
    "%%file src/81.py\n",
    "import pycountry\n",
    "country_list = [country.name for country in pycountry.countries if len(country.name.split()) > 1]\n",
    "\n",
    "with open('work/80.txt') as f_r:\n",
    "    for line in f_r:\n",
    "        for country in country_list:\n",
    "            if country in line:\n",
    "                line = line.replace(country, country.replace(' ', '_'))\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python src/81.py > work/81.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The term anarchism is a compound word composed from the word anarchy and the suffix -ism themselves derived respectively from the Greek i.e anarchy from anarchos meaning one without rulers from the privative prefix ἀν- an- i.e without and archos i.e leader ruler cf archon or arkhē i.e authority sovereignty realm magistracy and the suffix or -ismos -isma from the verbal infinitive suffix -ίζειν -izein The first known use of this word was in 1539.\"Anarchist was the term adopted by Maximilien de Robespierre to attack those on the left whom he had used for his own ends during the French Revolution but was determined to get rid of though among these anarchists there were few who exhibited the social revolt characteristics of later anarchists There would be many revolutionaries of the early nineteenth century who contributed to the anarchist doctrines of the next generation such as William Godwin and Wilhelm Weitling but they did not use the word anarchist or anarchism in describing themselves or their beliefs Pierre-Joseph Proudhon was the first political philosopher to call himself an anarchist marking the formal birth of anarchism in the mid-nineteenth century Since the 1890s from France the term libertarianism has often been used as a synonym for anarchism and was used almost exclusively in this sense until the 1950s in the United_States its use as a synonym is still common outside the United_States On the other hand some use libertarianism to refer to individualistic free-market philosophy only referring to free-market anarchism as libertarian anarchism\r\n",
      "grep: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!grep 'United_States' work/81.txt | head -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 82. 文脈の抽出\n",
    "81で作成したコーパス中に出現するすべての単語ttに関して，単語ttと文脈語ccのペアをタブ区切り形式ですべて書き出せ．ただし，文脈語の定義は次の通りとする．\n",
    "\n",
    "* ある単語ttの前後dd単語を文脈語ccとして抽出する（ただし，文脈語に単語ttそのものは含まない）\n",
    "* 単語ttを選ぶ度に，文脈幅ddは{1,2,3,4,5}{1,2,3,4,5}の範囲でランダムに決める．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 時間計測\n",
    "10万行に切ったファイルで実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.94349002838135sec\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from time import time\n",
    "random.seed(114514)\n",
    "\n",
    "start = time()\n",
    "with open('work/81.txt') as f_r:\n",
    "    for line in f_r:\n",
    "        words = line.split()\n",
    "        for i, word in enumerate(words):\n",
    "            window_size = random.randint(1, 5)\n",
    "            contexts = words[max(i-window_size, 0):i] + words[i+1:i+window_size+1]\n",
    "print('{}sec'.format(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.472185134887695sec\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "random.seed(114514)\n",
    "\n",
    "core_num = multiprocessing.cpu_count() // 2\n",
    "start = time()\n",
    "def proc(line):\n",
    "    words = line.split()\n",
    "    for i, word in enumerate(words):\n",
    "        window_size = random.randint(1, 5)\n",
    "        contexts = words[max(i-window_size, 0):i] + words[i+1:i+window_size+1]\n",
    "\n",
    "with open('work/81.txt') as f_r:\n",
    "    Parallel(n_jobs=core_num)(delayed(proc)(line) for line in f_r)\n",
    "print('{}sec'.format(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/82.py\n"
     ]
    }
   ],
   "source": [
    "%%file src/82.py\n",
    "import random\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "random.seed(114514)\n",
    "\n",
    "core_num = multiprocessing.cpu_count() // 2\n",
    "def proc_82(line):\n",
    "    words = line.split()\n",
    "    for i, word in enumerate(words):\n",
    "        window_size = random.randint(1, 5)\n",
    "        contexts = words[max(i-window_size, 0):i] + words[i+1:i+window_size+1]\n",
    "        for context in contexts:\n",
    "            print('{}\\t{}'.format(word, context))\n",
    "\n",
    "with open('work/81.txt') as f_r:\n",
    "    Parallel(n_jobs=core_num)(delayed(proc_82)(line) for line in f_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python src/82.py > work/82.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism\tis\r\n",
      "is\tAnarchism\r\n",
      "is\ta\r\n",
      "is\tpolitical\r\n",
      "is\tphilosophy\r\n",
      "a\tAnarchism\r\n",
      "a\tis\r\n",
      "a\tpolitical\r\n",
      "a\tphilosophy\r\n",
      "a\tthat\r\n"
     ]
    }
   ],
   "source": [
    "!head work/82.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 83. 単語／文脈の頻度の計測\n",
    "82の出力を利用し，以下の出現分布，および定数を求めよ．\n",
    "\n",
    "* f(t,c)f(t,c): 単語ttと文脈語ccの共起回数\n",
    "* f(t,∗)f(t,∗): 単語ttの出現回数\n",
    "* f(∗,c)f(∗,c): 文脈語ccの出現回数\n",
    "* NN: 単語と文脈語のペアの総出現回数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メモリ使用量計測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/83_memcheck1.py\n"
     ]
    }
   ],
   "source": [
    "%%file src/83_memcheck1.py\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "#愚直に数え上げる\n",
    "#Counterよりdefaultdictの方が早いらしい http://hateda.hatenadiary.jp/entry/2015/03/19/223638\n",
    "#10万行に切ったファイルをそのまま読み込んだ場合のメモリ使用量\n",
    "@profile\n",
    "def main():\n",
    "    f_t = defaultdict(int)\n",
    "    f_c = defaultdict(int)\n",
    "    f_t_c = defaultdict(int)\n",
    "    with open('work/82_.txt') as f_r:\n",
    "        for line in f_r:\n",
    "            word, context = line.split('\\t')\n",
    "            word = word.rstrip()\n",
    "            context = context.rstrip()\n",
    "            f_t[word] += 1\n",
    "            f_c[context] += 1\n",
    "            f_t_c[(word, context)] += 1\n",
    "    print('N={}'.format(len(f_t_c)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=72367\r\n",
      "Filename: src/83_memcheck1.py\r\n",
      "\r\n",
      "Line #    Mem usage    Increment   Line Contents\r\n",
      "================================================\r\n",
      "     4   39.652 MiB    0.000 MiB   @profile\r\n",
      "     5                             def main():\r\n",
      "     6   39.652 MiB    0.000 MiB       f_t = defaultdict(int)\r\n",
      "     7   39.652 MiB    0.000 MiB       f_c = defaultdict(int)\r\n",
      "     8   39.652 MiB    0.000 MiB       f_t_c = defaultdict(int)\r\n",
      "     9   39.652 MiB    0.000 MiB       with open('work/82_.txt') as f_r:\r\n",
      "    10   56.895 MiB   17.242 MiB           for line in f_r:\r\n",
      "    11   56.895 MiB    0.000 MiB               word, context = line.split('\\t')\r\n",
      "    12   56.895 MiB    0.000 MiB               word = word.rstrip()\r\n",
      "    13   56.895 MiB    0.000 MiB               context = context.rstrip()\r\n",
      "    14   56.895 MiB    0.000 MiB               f_t[word] += 1\r\n",
      "    15   56.895 MiB    0.000 MiB               f_c[context] += 1\r\n",
      "    16   56.895 MiB    0.000 MiB               f_t_c[(word, context)] += 1\r\n",
      "    17   56.898 MiB    0.004 MiB       print('N={}'.format(len(f_t_c)))\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python -m memory_profiler src/83_memcheck1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/83_memcheck2.py\n"
     ]
    }
   ],
   "source": [
    "%%file src/83_memcheck2.py\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "def chunked(iterable, n):\n",
    "    return [iterable[x:x + n] for x in range(0, len(iterable), n)]\n",
    "\n",
    "#時間を犠牲にメモリに優しく\n",
    "#10万行で切ったファイルを10倍の時間をかけて読んだ場合のメモリ使用量\n",
    "@profile\n",
    "def main():\n",
    "    f_t = defaultdict(int)\n",
    "    f_c = defaultdict(int)\n",
    "\n",
    "    with open('work/82_.txt') as f_r:\n",
    "        for line in f_r:\n",
    "            word, context = line.split('\\t')\n",
    "            word = word.rstrip()\n",
    "            context = context.rstrip()\n",
    "            f_t[word] += 1\n",
    "            f_c[context] += 1\n",
    "\n",
    "    word_list = [key for key in f_t.keys()]\n",
    "    splited_list = chunked(word_list, len(word_list)//10)\n",
    "\n",
    "    for splited in splited_list:\n",
    "        f_t_c = defaultdict(int)\n",
    "        with open('work/82_.txt') as f_r:\n",
    "            for line in f_r:\n",
    "                word, context = line.split('\\t')\n",
    "                word = word.rstrip()\n",
    "                context = context.rstrip()\n",
    "                if word in splited:\n",
    "                    f_t_c[(word, context)] += 1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: src/83_memcheck2.py\r\n",
      "\r\n",
      "Line #    Mem usage    Increment   Line Contents\r\n",
      "================================================\r\n",
      "     7   39.477 MiB    0.000 MiB   @profile\r\n",
      "     8                             def main():\r\n",
      "     9   39.477 MiB    0.000 MiB       f_t = defaultdict(int)\r\n",
      "    10   39.477 MiB    0.000 MiB       f_c = defaultdict(int)\r\n",
      "    11                             \r\n",
      "    12   39.480 MiB    0.004 MiB       with open('work/82_.txt') as f_r:\r\n",
      "    13   40.316 MiB    0.836 MiB           for line in f_r:\r\n",
      "    14   40.316 MiB    0.000 MiB               word, context = line.split('\\t')\r\n",
      "    15   40.316 MiB    0.000 MiB               word = word.rstrip()\r\n",
      "    16   40.316 MiB    0.000 MiB               context = context.rstrip()\r\n",
      "    17   40.316 MiB    0.000 MiB               f_t[word] += 1\r\n",
      "    18   40.316 MiB    0.000 MiB               f_c[context] += 1\r\n",
      "    19                             \r\n",
      "    20   40.316 MiB    0.000 MiB       word_list = [key for key in f_t.keys()]\r\n",
      "    21   40.316 MiB    0.000 MiB       splited_list = chunked(word_list, len(word_list)//10)\r\n",
      "    22                             \r\n",
      "    23   42.270 MiB    1.953 MiB       for splited in splited_list:\r\n",
      "    24   42.270 MiB    0.000 MiB           f_t_c = defaultdict(int)\r\n",
      "    25   42.270 MiB    0.000 MiB           with open('work/82_.txt') as f_r:\r\n",
      "    26   42.270 MiB    0.000 MiB               for line in f_r:\r\n",
      "    27   42.270 MiB    0.000 MiB                   word, context = line.split('\\t')\r\n",
      "    28   42.270 MiB    0.000 MiB                   word = word.rstrip()\r\n",
      "    29   42.270 MiB    0.000 MiB                   context = context.rstrip()\r\n",
      "    30   42.270 MiB    0.000 MiB                   if word in splited:\r\n",
      "    31   42.270 MiB    0.000 MiB                       f_t_c[(word, context)] += 1\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python -m memory_profiler src/83_memcheck2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=129320712\n"
     ]
    }
   ],
   "source": [
    "#素直に実装してるつもりなのに余裕で7GB超える 岡崎さんの素直[要出典]とは\n",
    "#時期が悪すぎてサーバー激混み\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "f_t = defaultdict(int)\n",
    "f_c = defaultdict(int)\n",
    "f_t_c = defaultdict(int)\n",
    "\n",
    "with open('work/82.txt') as f_r:\n",
    "    for line in f_r:\n",
    "        word, context = line.split('\\t')\n",
    "        word = word.rstrip()\n",
    "        context = context.rstrip()\n",
    "        f_t[word] += 1\n",
    "        f_c[context] += 1\n",
    "        f_t_c[(word, context)] += 1\n",
    "\n",
    "with open('work/f_t.pickle', 'wb') as f, open('work/f_c.pickle', 'wb') as g, open('work/f_t_c.pickle', 'wb') as h:\n",
    "    pickle.dump(f_t, f)\n",
    "    pickle.dump(f_c, g)\n",
    "    pickle.dump(f_t_c, h)\n",
    "with open('work/N.pickle', 'wb') as i:\n",
    "    pickle.dump(len(f_t_c), i)\n",
    "\n",
    "# with open('work/f_t.txt', 'w') as f, open('work/f_c.txt', 'w') as g, open('work/f_t_c.txt', 'w') as h:\n",
    "#     for i, j in f_t.items():\n",
    "#         f.write('{}\\t{}\\n'.format(i, j))\n",
    "#     for i, j in f_c.items():\n",
    "#         g.write('{}\\t{}\\n'.format(i, j))\n",
    "#     for i, j in f_t_c.items():\n",
    "#         h.write('{}\\t{}\\n'.format(i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-12:\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-9:\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-7:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Exception in thread Thread-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/pool.py\", line 445, in _handle_results\n",
      "    cache[job]._set(i, obj)\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/pool.py\", line 613, in _set\n",
      "    self._callback(self._value)\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/joblib/parallel.py\", line 219, in __call__\n",
      "    self.parallel.dispatch_next()\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/joblib/parallel.py\", line 599, in dispatch_next\n",
      "    if not self.dispatch_one_batch(self._original_iterator):\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/joblib/parallel.py\", line 625, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/joblib/parallel.py\", line 588, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/joblib/_parallel_backends.py\", line 141, in apply_async\n",
      "    return self._pool.apply_async(SafeFunction(func), callback=callback)\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/pool.py\", line 322, in apply_async\n",
      "    raise ValueError(\"Pool not running\")\n",
      "ValueError: Pool not running\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/joblib/pool.py\", line 360, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/joblib/pool.py\", line 362, in get\n",
      "    return recv()\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-75f126f12587>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'work/82.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf_r\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcore_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc_83\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'work/f_t.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'work/f_c.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'work/f_t_c.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;31m# we empty it and Python list are not thread-safe by default hence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0;31m# the use of the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m                 \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#並列にしてるはずがめちゃくちゃ遅くなる 辛い\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "\n",
    "f_t = defaultdict(int)\n",
    "f_c = defaultdict(int)\n",
    "f_t_c = defaultdict(int)\n",
    "\n",
    "core_num = multiprocessing.cpu_count() // 2\n",
    "\n",
    "def proc_83(line):\n",
    "    word, context = line.split('\\t')\n",
    "    word = word.rstrip()\n",
    "    context = context.rstrip()\n",
    "    f_t[word] += 1\n",
    "    f_c[context] += 1\n",
    "    f_t_c[(word, context)] += 1\n",
    "\n",
    "with open('work/82.txt') as f_r:\n",
    "    Parallel(n_jobs=core_num)(delayed(proc_83)(line) for line in f_r)\n",
    "        \n",
    "with open('work/f_t.pickle', 'wb') as f, open('work/f_c.pickle', 'wb') as g, open('work/f_t_c.pickle', 'wb') as h:\n",
    "    pickle.dump(f_t, f)\n",
    "    pickle.dump(f_c, g)\n",
    "    pickle.dump(f_t_c, h)\n",
    "with open('work/N.pickle', 'wb') as i:\n",
    "    pickle.dump(len(f_t_c), i)\n",
    "        \n",
    "# with open('work/f_t.txt', 'w') as f, open('work/f_c.txt', 'w') as g, open('work/f_t_c.txt', 'w') as h:\n",
    "#     for i, j in f_t.items():\n",
    "#         f.write('{}\\t{}\\n'.format(i, j))\n",
    "#     for i, j in f_c.items():\n",
    "#         g.write('{}\\t{}\\n'.format(i, j))\n",
    "#     for i, j in f_t_c.items():\n",
    "#         h.write('{}\\t{}\\n'.format(i, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 84. 単語文脈行列の作成\n",
    "83の出力を利用し，単語文脈行列XXを作成せよ．ただし，行列XXの各要素XtcXtcは次のように定義する．\n",
    "\n",
    "* f(t,c)≥10f(t,c)≥10ならば，Xtc=PPMI(t,c)=max{logN×f(t,c)f(t,∗)×f(∗,c),0}Xtc=PPMI(t,c)=max{log⁡N×f(t,c)f(t,∗)×f(∗,c),0}\n",
    "* f(t,c)<10f(t,c)<10ならば，Xtc=0Xtc=0\n",
    "ここで，PPMI(t,c)PPMI(t,c)はPositive Pointwise Mutual Information（正の相互情報量）と呼ばれる統計量である．なお，行列XXの行数・列数は数百万オーダとなり，行列のすべての要素を主記憶上に載せることは無理なので注意すること．幸い，行列XXのほとんどの要素は00になるので，非00の要素だけを書き出せばよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "with open('work/f_t.pickle', mode='rb') as f:\n",
    "    f_t = pickle.load(f)\n",
    "with open('work/f_c.pickle', mode='rb') as g:\n",
    "    f_c = pickle.load(g)\n",
    "with open('work/f_t_c.pickle', mode='rb') as h:\n",
    "    f_t_c = pickle.load(h)\n",
    "with open('work/N.pickle', mode='rb') as i:\n",
    "    N = pickle.load(i)\n",
    "    \n",
    "idx = {word:num for num, word in enumerate(f_t.keys())}\n",
    "idx_size = len(idx)\n",
    "f_matrix = sp.lil_matrix((idx_size, idx_size))\n",
    "\n",
    "for t_c, f_t_c_num in f_t_c.items():\n",
    "    if f_t_c_num > 9:\n",
    "        f_matrix[idx[t_c[0]], idx[t_c[1]]] = max(np.log10(N*f_t_c_num*1.0 / (f_c[t_c[1]] * f_t[t_c[0]])*1.0), 0)\n",
    "    \n",
    "with open('work/84.pickle', 'wb') as f:\n",
    "    pickle.dump(f_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method spmatrix.nonzero of <1808218x1808218 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1231308 stored elements in LInked List format>>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_matrix.nonzero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 85. 主成分分析による次元圧縮\n",
    "84で得られた単語文脈行列に対して，主成分分析を適用し，単語の意味ベクトルを300次元に圧縮せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pickle\n",
    "\n",
    "with open('work/84.pickle', mode = 'rb') as f, open('work/85.pickle', 'wb') as g:\n",
    "    compressed_matrix = TruncatedSVD(300).fit_transform(pickle.load(f))\n",
    "    pickle.dump(compressed_matrix, g, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 86. 単語ベクトルの表示\n",
    "85で得た単語の意味ベクトルを読み込み，\"United States\"のベクトルを表示せよ．ただし，\"United States\"は内部的には\"United_States\"と表現されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('work/85.pickle', mode = 'rb') as f, open('work/f_t.pickle', mode='rb') as g:\n",
    "    compressed_matrix = pickle.load(f)\n",
    "    f_t = pickle.load(g)\n",
    "    idx = {word:num for num, word in enumerate(f_t.keys())}\n",
    "    \n",
    "def word_to_vec(word):\n",
    "    if word in idx:\n",
    "        return(compressed_matrix[idx[word]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.22608626e-01,   1.33169412e+00,  -2.91147759e-01,\n",
       "        -5.97374155e-02,   2.48949822e-01,  -2.87291890e-01,\n",
       "         4.50835228e-01,  -2.30375890e-01,  -4.88754087e-02,\n",
       "        -5.83240878e-01,   1.98315105e-01,  -8.47092755e-03,\n",
       "        -8.96536295e-01,  -6.04573734e-02,  -8.74321141e-01,\n",
       "         3.56227635e-01,  -1.16886495e+00,  -1.40522692e-01,\n",
       "         6.35137278e-01,   9.92096985e-02,   4.01139757e-01,\n",
       "        -4.67382090e-01,  -4.79606936e-01,   5.78501914e-02,\n",
       "        -7.41588521e-03,   1.60342639e-01,   1.90910175e-01,\n",
       "        -3.97434763e-02,   1.45836810e-01,   8.94296560e-02,\n",
       "        -3.05635787e-01,   6.83708832e-02,   5.62568507e-02,\n",
       "        -1.14634595e-01,  -1.46208405e-01,  -9.55660159e-02,\n",
       "        -7.84077407e-02,   5.19851641e-02,  -1.67811956e-01,\n",
       "         2.05772023e-01,   2.62542009e-01,   2.07036984e-03,\n",
       "         3.34689546e-02,  -1.32484455e-01,   1.96644234e-01,\n",
       "         1.54922464e-01,   2.89121211e-01,  -2.19886982e-01,\n",
       "         2.29096847e-01,  -3.76430097e-01,   5.02962679e-01,\n",
       "         1.00506500e-01,   5.41156760e-02,   9.89518909e-02,\n",
       "        -3.00856253e-01,   5.06480425e-02,   1.46157010e-01,\n",
       "         1.11364329e-01,   2.15007040e-01,  -3.70024790e-01,\n",
       "        -4.16206543e-01,   6.52791541e-01,  -6.35004513e-01,\n",
       "        -6.06539070e-01,   6.37398179e-01,   3.22639832e-02,\n",
       "         2.77663821e-02,   4.53277599e-01,   8.60462945e-02,\n",
       "        -4.30917637e-01,  -1.27931120e-01,  -1.76208486e-01,\n",
       "        -4.04161990e-02,  -5.84099797e-02,  -1.07839680e-01,\n",
       "        -4.01194947e-02,   2.15693761e-01,   9.90431425e-02,\n",
       "         7.35324094e-02,  -1.21458509e-01,   3.04959091e-01,\n",
       "        -1.01222591e-01,  -2.61919501e-01,   3.17104959e-01,\n",
       "        -4.21695889e-01,  -4.46645609e-02,  -2.16054375e-01,\n",
       "        -2.41797546e-03,   1.39345658e-04,  -2.03187275e-01,\n",
       "         3.84652535e-01,   5.97835778e-02,  -2.38580526e-01,\n",
       "        -1.59314728e-02,   1.62826247e-01,   4.69374387e-01,\n",
       "         2.13646111e-01,   1.17905776e-02,   1.04513906e-01,\n",
       "         2.00361216e-03,   2.65343325e-01,  -3.37862853e-02,\n",
       "        -1.46056833e-01,   2.12424667e-02,   1.70707137e-01,\n",
       "         2.00191569e-01,   3.07024324e-02,  -4.66393374e-02,\n",
       "         2.01099110e-03,   6.45651942e-02,  -1.04927800e-01,\n",
       "        -6.13808331e-02,  -1.58297897e-01,   1.30910126e-01,\n",
       "        -3.83531970e-01,   4.86726539e-02,  -2.47634391e-01,\n",
       "        -3.68951200e-01,   6.02773628e-02,   4.70769130e-01,\n",
       "         3.87270758e-04,   1.87504499e-01,   3.37890342e-01,\n",
       "        -3.15561141e-01,   1.08101991e-01,  -2.26545179e-01,\n",
       "        -3.66676222e-01,   1.67809300e-01,  -2.78941827e-01,\n",
       "         4.92239310e-02,   3.49350363e-02,  -4.70561772e-02,\n",
       "         1.64094956e-01,   7.66214911e-03,   3.85034226e-02,\n",
       "         8.69063869e-02,   1.82328505e-01,  -7.59486646e-02,\n",
       "        -2.26301081e-01,   3.54187440e-01,  -2.68179938e-01,\n",
       "        -1.53956777e-01,   3.15395007e-01,  -2.96291672e-01,\n",
       "         3.58844744e-01,  -1.33955237e-01,  -1.16464291e-01,\n",
       "        -4.38452242e-01,  -3.39492895e-01,  -9.84664268e-02,\n",
       "        -1.76315330e-01,  -2.49692602e-01,  -1.98471721e-01,\n",
       "         3.50622324e-01,  -1.60198740e-01,  -6.87392897e-02,\n",
       "        -1.86851937e-01,  -1.48695765e-01,  -2.44309960e-01,\n",
       "         2.60323764e-02,  -1.35179756e-03,   6.71151803e-02,\n",
       "         2.58351292e-02,  -2.16962933e-02,   2.19093128e-01,\n",
       "        -1.85178705e-01,   2.79979942e-01,   2.81017218e-01,\n",
       "         3.66555555e-02,  -1.02932643e-01,  -1.74114601e-01,\n",
       "        -2.18930379e-01,  -1.59403120e-02,   3.50346562e-01,\n",
       "        -1.46846851e-01,  -1.70181634e-01,  -1.23940893e-01,\n",
       "         3.65314568e-01,  -1.70777582e-01,   1.90258724e-01,\n",
       "         6.12182705e-02,  -6.65011103e-02,  -2.54668594e-01,\n",
       "         2.73293995e-01,  -5.19765792e-02,  -1.08029618e-01,\n",
       "        -8.15404001e-02,  -8.54778997e-02,  -1.10077135e-01,\n",
       "         6.67151572e-02,   2.59982176e-01,   1.00257199e-01,\n",
       "         1.39274363e-01,   5.16417951e-02,  -9.70252124e-02,\n",
       "         6.50302220e-02,   4.06324147e-02,  -1.32724409e-01,\n",
       "        -9.84963650e-03,   2.29231031e-01,  -4.04949905e-01,\n",
       "         1.64868432e-01,  -2.93523321e-02,   2.54746096e-03,\n",
       "        -1.24832301e-01,  -1.34980922e-01,  -1.06859535e-01,\n",
       "         4.55699765e-03,   5.62275985e-02,  -7.00287932e-02,\n",
       "         1.89303225e-01,   1.87187179e-02,   7.15773427e-02,\n",
       "        -7.59921597e-02,  -3.01103514e-01,   2.29523442e-01,\n",
       "        -9.13418539e-02,  -2.25389585e-01,  -9.88982776e-02,\n",
       "        -2.18017910e-02,  -2.74353054e-02,  -1.50924918e-01,\n",
       "         5.51683060e-03,   1.56736532e-02,   1.53648183e-01,\n",
       "         9.24840233e-02,   2.06428734e-01,   8.59501193e-02,\n",
       "        -2.83648541e-01,   5.78514692e-02,   2.24353404e-02,\n",
       "        -1.47828936e-01,   1.25196434e-01,  -1.80539081e-02,\n",
       "         2.30269589e-01,  -8.25064736e-02,  -1.87287062e-01,\n",
       "        -4.22642796e-02,  -4.19830450e-02,   1.84494608e-01,\n",
       "         1.30030610e-01,   3.74744942e-02,  -1.86641918e-01,\n",
       "        -1.88479969e-01,  -1.61297599e-01,   8.20187921e-02,\n",
       "         5.42624952e-02,   1.57402926e-01,   7.22392626e-02,\n",
       "         2.15052998e-01,  -5.54083060e-02,  -3.92924945e-02,\n",
       "         2.36652257e-01,  -5.88531697e-02,  -3.28743624e-02,\n",
       "         1.20101637e-01,   9.71167848e-02,   1.45401495e-02,\n",
       "         4.72241454e-02,   1.52582559e-01,   7.38394706e-02,\n",
       "        -1.95370816e-02,   1.07940595e-01,   2.70398487e-01,\n",
       "         1.93127559e-02,  -3.65198217e-01,  -8.93935978e-02,\n",
       "        -4.60028055e-02,  -5.28735389e-02,  -7.76667206e-02,\n",
       "        -4.20969040e-02,   1.34675926e-01,  -7.09287075e-02,\n",
       "         1.26913529e-02,   2.31463399e-01,   1.24794210e-01,\n",
       "         3.78144947e-02,  -7.35805297e-03,  -1.63649843e-01,\n",
       "        -1.79018811e-01,   7.44196684e-02,   6.18653121e-02,\n",
       "         1.11530391e-01,  -5.31806997e-02,  -9.97655411e-02,\n",
       "        -2.65813500e-01,   6.04395179e-02,   5.74140335e-02,\n",
       "         4.31245217e-02,  -8.24933509e-02,   6.45248610e-02,\n",
       "        -1.73207269e-01,  -2.38830449e-02,  -1.03948456e-01,\n",
       "        -1.86704169e-01,   2.64887621e-01,  -1.99736203e-01,\n",
       "        -8.63160400e-02,   6.53175719e-02,   5.11947847e-02])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_vec('United_States')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 87. 単語の類似度\n",
    "85で得た単語の意味ベクトルを読み込み，\"United States\"と\"U.S.\"のコサイン類似度を計算せよ．ただし，\"U.S.\"は内部的に\"U.S\"と表現されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.metrics.pairwise import cosine_similarity こっちだと2D array云々でエラーでてめんどい\n",
    "import scipy.spatial.distance as scd\n",
    "def calc_cos_similarity(word1, word2):\n",
    "    vec1 = word_to_vec(word1)\n",
    "    vec2 = word_to_vec(word2)\n",
    "    if vec1 is not None and vec2 is not None:\n",
    "        return(1 - scd.cosine(vec1, vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81166901944255931"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_cos_similarity('United_States', 'U.S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 88. 類似度の高い単語10件\n",
    "85で得た単語の意味ベクトルを読み込み，\"England\"とコサイン類似度が高い10語と，その類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def get_top10_similar_words(word):\n",
    "    result = []\n",
    "    for word1 in idx.keys():\n",
    "        cos_sim = calc_cos_similarity(word, word1)\n",
    "        if cos_sim > 0 and word1 != word:\n",
    "            result.append([word1, cos_sim])\n",
    "    result.sort(key=lambda x:x[1], reverse = True)\n",
    "    pprint(result[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/scipy/spatial/distance.py:505: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['selectors', 0.82924151040292426],\n",
      " ['Herefordshire', 0.82473426808423278],\n",
      " ['Cumbria', 0.77641631845756975],\n",
      " ['Devizes', 0.74908334645347308],\n",
      " ['Quays', 0.74539005033160421],\n",
      " ['Ashbourne', 0.73946964355800593],\n",
      " ['Normans', 0.73607639443364592],\n",
      " ['Wolds', 0.73335782919381187],\n",
      " ['Woking', 0.7302595609901853],\n",
      " ['Huntingdonshire', 0.72756787628291919]]\n"
     ]
    }
   ],
   "source": [
    "get_top10_similar_words('England')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 89. 加法構成性によるアナロジー\n",
    "85で得た単語の意味ベクトルを読み込み，vec(\"Spain\") - vec(\"Madrid\") + vec(\"Athens\")を計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.metrics.pairwise import cosine_similarity こっちだと2D array云々でエラーでてめんどい\n",
    "import scipy.spatial.distance as scd\n",
    "def calc_cos_similarity_(vec, word):\n",
    "    vec1 = word_to_vec(word)\n",
    "    if vec is not None and vec1 is not None:\n",
    "        return(1 - scd.cosine(vec, vec1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def get_top10_similar_words(word):\n",
    "    result = []\n",
    "    for word1 in idx.keys():\n",
    "        cos_sim = calc_cos_similarity_(word, word1)\n",
    "        if cos_sim > 0 and word1 != word:\n",
    "            result.append([word1, cos_sim])\n",
    "    result.sort(key=lambda x:x[1], reverse = True)\n",
    "    pprint(result[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  import sys\n",
      "/home/tagami/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/scipy/spatial/distance.py:505: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Spain', 0.84438047910663439],\n",
      " ['Portugal', 0.80572639343589325],\n",
      " ['Denmark', 0.79830433842360948],\n",
      " ['Grandee', 0.79705620741072691],\n",
      " ['Malaga', 0.79704842007566534],\n",
      " ['Cadiz', 0.79704842007566523],\n",
      " ['Andalusia', 0.79704842007534704],\n",
      " ['Málaga', 0.79465967720933017],\n",
      " ['Belgium', 0.79115157348938558],\n",
      " ['Sweden', 0.78811707862648916]]\n"
     ]
    }
   ],
   "source": [
    "vec = word_to_vec('Spain') - word_to_vec('Madrid') + word_to_vec('Athens')\n",
    "get_top10_similar_words(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
